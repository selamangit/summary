###特征选择与稀疏学习
对一个学习任务来说，给定属性集，称这些属性为**特征**，其中有一些属性对当前学习任务来说很有效，这些属性称为**相关特征**，对当前任务没什么效果的属性，称为**无关特征**。还有一类特征称为**冗余特征**，它们所包含的特征能从其他的特征中推演出来，例如：立方体对象，已有特征“底面长”，“底面宽”，则“底面积”就是冗余特征，冗余特征在很多时候不起作用，但它有时能降低学习任务的难度（根据学习任务不同来决定，如：求立方体的体积时底面积就能使体积的估算更简单），若某个冗余特征恰好对应了完成学习任务所需的“中间概念”，则该冗余特征是有益的。
现在大部分机器学习任务中，特征选择占有重要的地位，特征选择是一个数据预处理的过程。
**特征选择对学习任务的好处**
其一：因为在机器学习任务中，当给定样本中属性过多时，会经常遇到维数灾难的问题，特征选择能选出其中重要的特征，之后再进行学习过程就能大为减轻维数灾难的问题。
其二：去除不相关的特征往往会降低学习任务的难度，将纷繁复杂的因素抽丝剥茧，只留下关键因素，则真相往往更容易看清。
**注意**：特征选择的过程中必须确保不丢失重要特征，否则后续的学习过程会因为重要信息的缺失而无法获得好的性能，而且特征选择是对当前学习任务而言的，不同的学习任务，相关特征的选择不同
####子集搜索问题
对于给定的特征集合，将每个特征看作一个个**候选子集**，接着利用信息增益的方法来评价每个候选子集的好坏，接着将评出好的那个特征加入到上一轮中的每个候选子集中，再进行新一轮的评价，直到无法找到更好的候选子集为止。
**前向搜索**
先将给定的特征集合，分为一个个单特征的候选子集，进行评价，将评好的特征作为选定集，接着将选定集中的特征与剩余的候选子集分别组合，再一一进行评价，将评价好的特征子集组合作为选定集。假设在第k+1轮时最优的（k+1）特征子集组合不如上一轮的选定集，则停止子集搜索选取上一轮的选定集作为特征选择的结果。
![](技术学习图片/11.png)
**后向搜索**
从完整的特征集合开始，每次尝试减去一个特征再评价，直到减去特征后的特征子集的评价结果不如减去前的特征集合时停止子集搜索，选取减去特征前的特征子集为特征选择的结果。
**双向搜索**
每一轮增加评价好的相关特征（这些特征在后续轮中不会被去除），减去评价不好的特征。
上述的策略都是贪心的，因为它们仅考虑了使本轮选定集最优，没有从全局方面去选择最合适的选择。
####子集评价
利用到了信息熵和信息增益的知识点